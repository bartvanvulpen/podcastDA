{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0306 00:42:27.453490 4617063936 file_utils.py:41] PyTorch version 1.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification,DistilBertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0306 00:42:32.385972 4617063936 configuration_utils.py:256] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /Users/Bart/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0306 00:42:32.387161 4617063936 configuration_utils.py:292] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"dim\": 768,\n",
      "  \"do_sample\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_labels\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tie_weights_\": true,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0306 00:42:32.879811 4617063936 modeling_utils.py:461] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at /Users/Bart/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I0306 00:42:34.807521 4617063936 modeling_utils.py:546] Weights of DistilBertForSequenceClassification not initialized from pretrained model: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "I0306 00:42:34.809116 4617063936 modeling_utils.py:552] Weights from pretrained model not used in DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "I0306 00:42:35.283277 4617063936 tokenization_utils.py:501] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/Bart/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=1)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0306 00:42:35.502650 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:35.573278 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1093 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:35.754170 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:35.872694 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:35.930357 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:36.083005 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:36.211015 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:36.974685 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (2518 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:37.069316 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:38.766727 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:38.975136 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:39.209563 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:39.394849 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:39.547132 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:39.894015 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:39.927110 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:40.506179 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:40.733573 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:41.675983 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:41.724028 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:41.948928 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1632 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:42.144492 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:42.493069 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:42.845165 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:43.074316 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1762 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:43.259757 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:43.290255 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:43.360323 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:43.407159 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:43.791268 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1637 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:43.888330 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:44.081105 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:44.282405 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (4240 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:44.499521 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0306 00:42:45.187283 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:45.384227 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:45.435225 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (998 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:45.533652 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:45.771664 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1530 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:45.824087 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:46.057579 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:46.091566 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:46.849785 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1396 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:47.075148 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:47.506669 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:47.632236 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:47.924450 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:48.153078 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:48.211351 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:48.646075 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:48.735615 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1398 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:48.790874 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:49.605330 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:49.832129 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:49.980090 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:50.225944 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1503 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:50.752519 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:50.959031 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:51.511736 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1636 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:51.817819 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:51.906490 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:51.943414 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:52.010196 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:52.378832 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:52.625654 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (2781 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:52.651360 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:52.725468 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:52.845255 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0306 00:42:52.897054 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:53.038861 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:53.454962 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1231 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:53.822661 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:53.936481 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:54.601130 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:54.638085 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:54.752429 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (2014 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:55.326122 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1254 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:55.556696 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:55.699250 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (2025 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:55.925266 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:56.174150 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1420 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:56.643674 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:56.693394 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:56.971141 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:57.162282 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:57.349850 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (2771 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:57.428837 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:57.477161 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:57.602028 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1317 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:57.761405 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:57.930637 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:58.094659 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:58.259407 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (1846 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:58.444302 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0306 00:42:58.588590 4617063936 tokenization_utils.py:1329] Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2945\n",
      "1264\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_utterances = train_df.text.apply(lambda x: tokenizer.encode(x,add_special_tokens=True))\n",
    "tokenized_utterances_test = test_df.text.apply(lambda x: tokenizer.encode(x,add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sentences for training set\n",
    "max_len = max(map(len,tokenized_utterances))\n",
    "padded_utterances = np.array([ i+[0]*(max_len-len(i))  for i in tokenized_utterances])\n",
    "attention_masked_utterances = np.where(padded_utterances != 0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sentences for test set\n",
    "padded_utterances_test = np.array([ i+[0]*(max_len-len(i))  for i in tokenized_utterances_test])\n",
    "attention_masked_utterances_test = np.where(padded_utterances_test != 0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors\n",
    "\n",
    "X_train = torch.tensor(padded_utterances)\n",
    "X_train_attention = torch.tensor(attention_masked_utterances)\n",
    "y_train = torch.tensor(np.array(train_df.sentiment_score.values)[:,np.newaxis], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(padded_utterances_test)\n",
    "X_test_attention = torch.tensor(attention_masked_utterances_test)\n",
    "y_test = torch.tensor(np.array(test_df.sentiment_score.values)[:,np.newaxis], dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(X_train, X_train_attention, y_train)\n",
    "train_loader = DataLoader(train_data,batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, X_test_attention)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6954, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-477936b699bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "optimizer =torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "for i in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for X_batch,X_attention_batch,y_batch in train_loader:\n",
    "        output = model(X_batch,attention_mask=X_attention_batch,labels=None)\n",
    "        y_pred = output[0]\n",
    "        loss = loss_fn(y_pred,y_batch)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
