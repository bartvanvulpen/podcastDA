{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impaired-democracy",
   "metadata": {},
   "source": [
    "## Niet runnen!\n",
    "\n",
    "Deze notebook is bedoeld voor tests en dingen die maar een keer gerund hoeven te worden, sommige cells duren meer dan een uur. Het uiteindelijke werk komt allemaal in semantic_orientation.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import operator\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.metrics import classification_report\n",
    "import zipfile\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\"podcast_data_no_audio/metadata/metadata.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths_for_en_episodes(subset_number):\n",
    "    \"\"\"\n",
    "    Function returns list of all paths to the json-files of english \n",
    "    episodes given subset number (bart: 0 , juno: 1, joris: 2)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    path1 = 'podcast_data_no_audio/podcasts-transcripts/' + str(subset_number)\n",
    "\n",
    "    folders = listdir(path1)\n",
    "\n",
    "    if '.DS_Store' in folders:\n",
    "        folders.remove('.DS_Store')\n",
    "\n",
    "    podcast_episodes_paths = []\n",
    "\n",
    "    for letter_or_number in tqdm(folders):    \n",
    "        path2 = path1 + '/' + letter_or_number\n",
    "\n",
    "\n",
    "        for show_uri in listdir(path2):\n",
    "            path3 = path2 + '/' + show_uri\n",
    "\n",
    "            # select english shows only\n",
    "            show_metadata = metadata_df.loc[metadata_df['show_filename_prefix'] == show_uri]\n",
    "\n",
    "            if len(show_metadata['language'].unique()) > 0:\n",
    "                if 'en' in show_metadata['language'].unique()[0]:\n",
    "                    for episode_uri in listdir(path3):\n",
    "                        path4 = path3 + '/' + episode_uri\n",
    "\n",
    "                        if '.json' in path4:\n",
    "                            podcast_episodes_paths.append(path4)\n",
    "\n",
    "                \n",
    "        \n",
    "    return len(podcast_episodes_paths), podcast_episodes_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_paths_for_en_episodes(1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_json_to_pandas(json_path):\n",
    "    \"\"\"\n",
    "    This function converts a podcast .json transcript into a \n",
    "    pandas dataframe with speaker tags, utterance text and open labels\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # get transcript parts from json file, remove empty parts\n",
    "    transcript_parts = []\n",
    "    for utt in data['results']:\n",
    "        try:\n",
    "            trans = utt['alternatives'][0]['transcript']\n",
    "        except KeyError:\n",
    "            trans = 0\n",
    "\n",
    "        if trans != 0:\n",
    "            transcript_parts.append(utt)\n",
    "    \n",
    "\n",
    "    # create list of sentences from dialogue\n",
    "    sentences = []\n",
    "    for index, utterance in enumerate(transcript_parts):\n",
    "\n",
    "        # get text of utterance\n",
    "        utterance_text = utterance['alternatives'][0]['transcript']\n",
    "        \n",
    "        # get sentences from text to split based on speakerTag\n",
    "        utterance_sentences = nltk.sent_tokenize(utterance_text)\n",
    "        for sent in utterance_sentences:\n",
    "            sent = sent.split(\" \")\n",
    "            if '' in sent:\n",
    "                sent.remove('')\n",
    "            sentences.append(sent)\n",
    "                \n",
    "    \n",
    "    # get words with tags from transcript file\n",
    "    words_with_tags = data['results'][-1]['alternatives'][0]['words']\n",
    "    \n",
    "    \n",
    "    # assign speakerTag to each sentence\n",
    "    # also fix mistakes when speakerTag switches to other speaker\n",
    "    # in the middle of a sentence\n",
    "    sentences_with_tags = []\n",
    "    \n",
    "    word_idx = 0\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        sent_with_tags = []\n",
    "        for word in sentence:\n",
    "            sent_with_tags.append((word, words_with_tags[word_idx]['speakerTag']))\n",
    "            word_idx += 1\n",
    "        \n",
    "        \n",
    "        c = Counter(elem[1] for elem in sent_with_tags)\n",
    "        sent_speakerTag = max(c.items(), key=operator.itemgetter(1))[0]\n",
    "        sentences_with_tags.append((' '.join(sentence), sent_speakerTag))\n",
    "        \n",
    "        \n",
    "    # merge sentences with same consecutive tags\n",
    "    utterances_texts = []\n",
    "    utterances_tags = []\n",
    "    merged_sents = []\n",
    "    for index, tagged_sent in enumerate(sentences_with_tags):\n",
    "\n",
    "        \n",
    "        # set initial value for tagged_sent\n",
    "        if index == 0:\n",
    "            curr_tag = tagged_sent[1]\n",
    "        \n",
    "        # speaker switch\n",
    "        if curr_tag != tagged_sent[1] and index > 0:\n",
    "            utterance_tag = merged_sents[0][1]\n",
    "            utterance_text = ' '.join([sent[0] for sent in merged_sents])\n",
    "            utterances_texts.append(utterance_text)\n",
    "            utterances_tags.append(utterance_tag)\n",
    "            merged_sents = []\n",
    "\n",
    "            \n",
    "        curr_tag = tagged_sent[1]\n",
    "        merged_sents.append(tagged_sent)\n",
    "        \n",
    "        \n",
    "        if index == len(sentences_with_tags)-1:\n",
    "            utterance_tag = merged_sents[0][1]\n",
    "            utterance_text = ' '.join([sent[0] for sent in merged_sents])\n",
    "            utterances_texts.append(utterance_text)\n",
    "            utterances_tags.append(utterance_tag)\n",
    "            \n",
    "   \n",
    "    # make utterances and tags are the same shape\n",
    "    if len(utterances_texts) == len(utterances_tags):\n",
    "        # create pandas dataframe\n",
    "        dialogue_df = pd.DataFrame(columns=['speaker_tag', 'text', 'sentiment_score'])\n",
    "\n",
    "        \n",
    "        # fill dataframe\n",
    "        for i, text in enumerate(utterances_texts): \n",
    "            dialogue_df.loc[i] = [utterances_tags[i]] + [text] + ['']\n",
    "\n",
    "    \n",
    "    return dialogue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the labeled .csv-files from Google Drive to use for validation.\n",
    "\n",
    "gdown.download('https://drive.google.com/uc?id=1aqE8yS7Lf8GfljmFEuW5pd3i5S2raW1B', 'separate_csv_files.zip', quiet=False)\n",
    "with zipfile.ZipFile('separate_csv_files.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "subsequent-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes all labeled data to binary values\n",
    "for filename in [f for f in listdir('labeled_datasets/separate_csv_files') \n",
    "                 if isfile(join('labeled_datasets/separate_csv_files', f))]:\n",
    "    path = 'labeled_datasets/separate_csv_files/' + filename\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    df.loc[df['sentiment_score'] == 0.0, 'sentiment_score'] = 1.0\n",
    "    df.loc[df['sentiment_score'] == -1.0, 'sentiment_score'] = 0.0\n",
    "    df.to_csv(path, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "computational-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all paths to the json-files of english episodes given subset number (bart: 0 , juno: 1, joris: 2)\n",
    "metadata_df = pd.read_csv(\"podcast_data_no_audio/metadata/metadata.tsv\",sep='\\t')\n",
    "\n",
    "\n",
    "def get_paths_for_en_episodes(subset_number):\n",
    "    \"\"\"\n",
    "    Function returns list of all paths to the json-files of english \n",
    "    episodes given subset number (bart: 0 , juno: 1, joris: 2)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    path1 = 'podcast_data_no_audio/podcasts-transcripts/' + str(subset_number)\n",
    "\n",
    "    folders = listdir(path1)\n",
    "\n",
    "    if '.DS_Store' in folders:\n",
    "        folders.remove('.DS_Store')\n",
    "\n",
    "    podcast_episodes_paths = []\n",
    "\n",
    "    for letter_or_number in tqdm(folders):    \n",
    "        path2 = path1 + '/' + letter_or_number\n",
    "\n",
    "\n",
    "        for show_uri in listdir(path2):\n",
    "            path3 = path2 + '/' + show_uri\n",
    "\n",
    "            # select english shows only\n",
    "            show_metadata = metadata_df.loc[metadata_df['show_filename_prefix'] == show_uri]\n",
    "\n",
    "            if len(show_metadata['language'].unique()) > 0:\n",
    "                if 'en' in show_metadata['language'].unique()[0]:\n",
    "                    for episode_uri in listdir(path3):\n",
    "                        path4 = path3 + '/' + episode_uri\n",
    "\n",
    "                        if '.json' in path4:\n",
    "                            podcast_episodes_paths.append(path4)\n",
    "\n",
    "                \n",
    "        \n",
    "    return len(podcast_episodes_paths), podcast_episodes_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "progressive-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_paths_for_en_episodes(1)[1]\n",
    "\n",
    "# Om een dict te maken met filenames als keys en de volledige transcripts als values\n",
    "\n",
    "folder_path = 'podcast_data_no_audio/podcasts-transcripts/1'\n",
    "# folder_path = 'podcast_data_no_audio/podcasts-transcripts/1/0/show_10AlBXJul8JZ5bREZUXBep'\n",
    "transcripts_dict = dict()\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if os.path.join(subdir, file)[-5:] == '.json':\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            data = json.load(open(filepath))\n",
    "            utterances = ''\n",
    "            \n",
    "            \n",
    "            for utterance_number, _ in enumerate(data['results']):  \n",
    "                utterance_dict = data['results'][utterance_number]['alternatives'][0]\n",
    "                try:\n",
    "                    utterance = utterance_dict['transcript'].strip()\n",
    "                    utterances += ' ' + utterance\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                \n",
    "\n",
    "            transcripts_dict[file] = utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Belangrijke functie voor representatie van een podcast episode ##\n",
    "\n",
    "def dialogue_json_to_pandas(json_path):\n",
    "    \"\"\"\n",
    "    This function converts a podcast .json transcript into a \n",
    "    pandas dataframe with speaker tags, utterance text and open labels\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # get transcript parts from json file, remove empty parts\n",
    "    transcript_parts = []\n",
    "    for utt in data['results']:\n",
    "        try:\n",
    "            trans = utt['alternatives'][0]['transcript']\n",
    "        except KeyError:\n",
    "            trans = 0\n",
    "\n",
    "        if trans != 0:\n",
    "            transcript_parts.append(utt)\n",
    "    \n",
    "\n",
    "    # create list of sentences from dialogue\n",
    "    sentences = []\n",
    "    for index, utterance in enumerate(transcript_parts):\n",
    "\n",
    "        # get text of utterance\n",
    "        utterance_text = utterance['alternatives'][0]['transcript']\n",
    "        \n",
    "        # get sentences from text to split based on speakerTag\n",
    "        utterance_sentences = nltk.sent_tokenize(utterance_text)\n",
    "        for sent in utterance_sentences:\n",
    "            sent = sent.split(\" \")\n",
    "            if '' in sent:\n",
    "                sent.remove('')\n",
    "            sentences.append(sent)\n",
    "                \n",
    "    \n",
    "    # get words with tags from transcript file\n",
    "    words_with_tags = data['results'][-1]['alternatives'][0]['words']\n",
    "    \n",
    "    \n",
    "    # assign speakerTag to each sentence\n",
    "    # also fix mistakes when speakerTag switches to other speaker\n",
    "    # in the middle of a sentence\n",
    "    sentences_with_tags = []\n",
    "    \n",
    "    word_idx = 0\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        sent_with_tags = []\n",
    "        for word in sentence:\n",
    "            sent_with_tags.append((word, words_with_tags[word_idx]['speakerTag']))\n",
    "            word_idx += 1\n",
    "        \n",
    "        \n",
    "        c = Counter(elem[1] for elem in sent_with_tags)\n",
    "        sent_speakerTag = max(c.items(), key=operator.itemgetter(1))[0]\n",
    "        sentences_with_tags.append((' '.join(sentence), sent_speakerTag))\n",
    "        \n",
    "        \n",
    "    # merge sentences with same consecutive tags\n",
    "    utterances_texts = []\n",
    "    utterances_tags = []\n",
    "    merged_sents = []\n",
    "    for index, tagged_sent in enumerate(sentences_with_tags):\n",
    "\n",
    "        \n",
    "        # set initial value for tagged_sent\n",
    "        if index == 0:\n",
    "            curr_tag = tagged_sent[1]\n",
    "        \n",
    "        # speaker switch\n",
    "        if curr_tag != tagged_sent[1] and index > 0:\n",
    "            utterance_tag = merged_sents[0][1]\n",
    "            utterance_text = ' '.join([sent[0] for sent in merged_sents])\n",
    "            utterances_texts.append(utterance_text)\n",
    "            utterances_tags.append(utterance_tag)\n",
    "            merged_sents = []\n",
    "\n",
    "            \n",
    "        curr_tag = tagged_sent[1]\n",
    "        merged_sents.append(tagged_sent)\n",
    "        \n",
    "        \n",
    "        if index == len(sentences_with_tags)-1:\n",
    "            utterance_tag = merged_sents[0][1]\n",
    "            utterance_text = ' '.join([sent[0] for sent in merged_sents])\n",
    "            utterances_texts.append(utterance_text)\n",
    "            utterances_tags.append(utterance_tag)\n",
    "            \n",
    "   \n",
    "    # make utterances and tags are the same shape\n",
    "    if len(utterances_texts) == len(utterances_tags):\n",
    "        # create pandas dataframe\n",
    "        dialogue_df = pd.DataFrame(columns=['speaker_tag', 'text', 'sentiment_score'])\n",
    "\n",
    "        \n",
    "        # fill dataframe\n",
    "        for i, text in enumerate(utterances_texts): \n",
    "            dialogue_df.loc[i] = [utterances_tags[i]] + [text] + ['']\n",
    "\n",
    "    \n",
    "    return dialogue_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "diagnostic-reviewer",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dialogue_json_to_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-80253ed1e308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_json_to_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0madd_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dialogue_json_to_pandas' is not defined"
     ]
    }
   ],
   "source": [
    "def add_scores(df):\n",
    "    NEAR = 1000\n",
    "    positive_words = ['good', 'great', 'nice', 'happy', 'easy', 'yes', 'yeah', 'love']\n",
    "    negative_words = ['hard', 'bad', 'wrong', 'tough', 'negative', 'no', 'shit', 'hate']\n",
    "    utt_lens = []\n",
    "    utt_scores = []\n",
    "    all_words = []\n",
    "    df_idx = 0\n",
    "\n",
    "\n",
    "    for utt in df.text:\n",
    "        utt_lens.append(len([w for w in nltk.word_tokenize(utt) if w not in ['.', ',', '?', '!', '\\'']]))\n",
    "        all_words += [w for w in nltk.word_tokenize(utt) if w not in ['.', ',', '?', '!', '\\'']]\n",
    "\n",
    "    word_counter = Counter([w.lower() for w in all_words])\n",
    "    positive_hits = sum([word_counter[p] for p in positive_words]) + 0.01\n",
    "    negative_hits = sum([word_counter[n] for n in negative_words]) + 0.01\n",
    "\n",
    "\n",
    "    for i in range(len(all_words) - 1):\n",
    "        if i >= sum(utt_lens[:df_idx+1]) or i == len(all_words) - 2:\n",
    "            if utt_scores:\n",
    "                df.sentiment_score[df_idx] = np.mean(utt_scores)\n",
    "            else:\n",
    "                df.sentiment_score[df_idx] = 0\n",
    "            df_idx += 1\n",
    "            utt_scores = []\n",
    "        if is_phrase(i, all_words):\n",
    "            neighbourhood = []\n",
    "            if NEAR > i:\n",
    "                neighbourhood += all_words[:i]\n",
    "            else:\n",
    "                neighbourhood += all_words[i-NEAR:i]\n",
    "            if i != len(all_words) - 2:\n",
    "                try:\n",
    "                    neighbourhood += all_words[i+2:i+2+NEAR]\n",
    "                except IndexError:\n",
    "                    neighbourhood += all_words[i+2:len(words)]\n",
    "\n",
    "\n",
    "            neighbourhood_counter = Counter([w.lower() for w in neighbourhood])\n",
    "            pos_neigh_hits = sum([neighbourhood_counter[p] for p in positive_words]) + 0.01\n",
    "            neg_neigh_hits = sum([neighbourhood_counter[n] for n in negative_words]) + 0.01\n",
    "\n",
    "\n",
    "            if pos_neigh_hits > 2 and neg_neigh_hits > 2:\n",
    "                score = np.log2((pos_neigh_hits * negative_hits)/ (neg_neigh_hits * positive_hits))\n",
    "                utt_scores.append(score)\n",
    "    df.sentiment_score /= max([max(df.sentiment_score), -min(df.sentiment_score)])\n",
    "    return df\n",
    "\n",
    "#     if orientations:\n",
    "#         plt.plot(np.arange(0, 1, 1/len(orientations)), orientations)\n",
    "# plt.show()\n",
    "\n",
    "df = dialogue_json_to_pandas(paths[10])\n",
    "add_scores(df).sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUD\n",
    "# Om semantic orientations van woorden te bepalen volgens 'Thumbs Up or Thumbs Down? Semantic Orientation \n",
    "# Applied to Unsupervised Classification of Reviews'\n",
    "\n",
    "NEAR = 500\n",
    "positive_words = ['good', 'great', 'nice', 'happy', 'easy']\n",
    "negative_words = ['hard', 'bad', 'wrong', 'tough', 'negative']\n",
    "\n",
    "\n",
    "for t in list(transcripts_dict.values())[:50]:\n",
    "    words = [w for w in nltk.word_tokenize(t) if w not in ['.', ',', '?', '!', '\\'']]\n",
    "    if len(words) > 10000:\n",
    "        orientations = []\n",
    "        word_counter = Counter([w.lower() for w in words])\n",
    "        positive_hits = sum([word_counter[p] for p in positive_words]) + 0.01\n",
    "        negative_hits = sum([word_counter[n] for n in negative_words]) + 0.01\n",
    "        \n",
    "        \n",
    "        for i in range(len(words) - 1):\n",
    "            if is_phrase(i, words):\n",
    "                neighbourhood = []\n",
    "                if NEAR > i:\n",
    "                    neighbourhood += words[:i]\n",
    "                else:\n",
    "                    neighbourhood += words[i-NEAR:i]\n",
    "                if i != len(words) - 2:\n",
    "                    try:\n",
    "                        neighbourhood += words[i+2:i+2+NEAR]\n",
    "                    except IndexError:\n",
    "                        neighbourhood += words[i+2:len(words)]\n",
    "                        \n",
    "                \n",
    "                neighbourhood_counter = Counter([w.lower() for w in neighbourhood])\n",
    "                pos_neigh_hits = sum([neighbourhood_counter[p] for p in positive_words]) + 0.01\n",
    "                neg_neigh_hits = sum([neighbourhood_counter[n] for n in negative_words]) + 0.01\n",
    "                \n",
    "                \n",
    "                if pos_neigh_hits > 2 and neg_neigh_hits > 2:\n",
    "                    o = np.log((pos_neigh_hits * negative_hits)/ (neg_neigh_hits * positive_hits))\n",
    "                    orientations.append(o)\n",
    "#                     print(words[i:i+2], o)\n",
    "#                     print(i, len(neighbourhood))\n",
    "    \n",
    "    \n",
    "        if orientations:\n",
    "            plt.plot(np.arange(0, 1, 1/len(orientations)), orientations)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_phrase(word_index, words):\n",
    "    first_tag = nltk.pos_tag([words[word_index]])[0][1]\n",
    "    second_tag = nltk.pos_tag([words[word_index + 1]])[0][1]\n",
    "    try:\n",
    "        third_tag = nltk.pos_tag([words[word_index + 2]])[0][1]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    if first_tag == 'JJ':\n",
    "        if second_tag in ['NN', 'NNS']:\n",
    "            return True\n",
    "        elif second_tag == 'JJ':\n",
    "            try:\n",
    "                if third_tag not in ['NN', 'NNS']:\n",
    "                    return True\n",
    "            except NameError:\n",
    "                return True\n",
    "    elif first_tag in ['RB', 'RBR', 'RBS']:\n",
    "        if second_tag == 'JJ':\n",
    "            try:\n",
    "                if third_tag not in ['NN', 'NNS']:\n",
    "                    return True\n",
    "            except NameError:\n",
    "                return True\n",
    "        elif second_tag in ['VB', 'VBD', 'VBN', 'VBG']:\n",
    "            return True\n",
    "    elif first_tag in ['NN', 'NNS']:\n",
    "        if second_tag == 'JJ':\n",
    "            try:\n",
    "                if third_tag not in ['NN', 'NNS']:\n",
    "                    return True\n",
    "            except NameError:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "recent-cleanup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61217"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Om de meest voorkomende alternatieven voor good/bad en excellent/poor te vinden\n",
    "\n",
    "# adjectives = []\n",
    "# c = 0\n",
    "\n",
    "\n",
    "# for t in transcripts_dict.values():\n",
    "#     c += 1\n",
    "#     if c % 100 == 0:\n",
    "#         print (str(c) + ' / ' + str(len(transcripts_dict)))\n",
    "#     words = [w for w in nltk.word_tokenize(t) if w not in['.', ',']]\n",
    "#     for i in range(2, len(words)-2):\n",
    "# #         if nltk.pos_tag([words[i]])[0][1] == 'NN':\n",
    "#         if any(w in ['not', 'n\\'t'] for w in [words[i-1].lower(), words[i-2].lower()]):\n",
    "#             adjectives.append('not ' + words[i].lower())\n",
    "#         else:\n",
    "#             adjectives.append(words[i].lower())\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # # adj_counter = Counter(adjectives).most_common(10000)\n",
    "# for w in [x[0] for x in adj_counter]:\n",
    "#     if nltk.pos_tag([w])[0][1] == 'NN' and nltk.pos_tag([w])[0][0] not in stopwords.words('english'):\n",
    "#         print(w)\n",
    "\n",
    "# count = Counter(adjectives)\n",
    "count['na']\n",
    "\n",
    "# # positive_words = ['good', 'great', 'nice', 'happy', 'easy', 'special', ]\n",
    "# # negative_words = ['hard', 'bad', 'wrong', 'tough', 'not good']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-metadata",
   "metadata": {},
   "source": [
    "# All words\n",
    "[('i', 2920664),\n",
    " ('the', 2707330),\n",
    " ('and', 2499693),\n",
    " ('you', 2267175),\n",
    " ('to', 2107211),\n",
    " ('that', 1910118),\n",
    " ('a', 1710804),\n",
    " ('it', 1694550),\n",
    " (\"'s\", 1628554),\n",
    " ('of', 1420169),\n",
    " ('like', 1362499),\n",
    " ('in', 939313),\n",
    " ('is', 936290),\n",
    " ('so', 885230),\n",
    " ('was', 806712),\n",
    " ('we', 765953),\n",
    " (\"n't\", 730109),\n",
    " ('do', 708999),\n",
    " ('this', 645240),\n",
    " ('know', 638151),\n",
    " ('but', 637519),\n",
    " ('just', 598106),\n",
    " ('for', 594334),\n",
    " ('they', 571416),\n",
    " ('yeah', 567540),\n",
    " ('on', 565937),\n",
    " ('he', 555150),\n",
    " ('what', 482788),\n",
    " ('have', 482146),\n",
    " (\"'re\", 475482),\n",
    " ('be', 456634),\n",
    " ('with', 444109),\n",
    " ('not', 435110),\n",
    " ('my', 423325),\n",
    " (\"'m\", 422080),\n",
    " ('there', 387415),\n",
    " ('are', 364117),\n",
    " ('your', 358892),\n",
    " ('?', 356301),\n",
    " ('because', 353328),\n",
    " ('all', 347258),\n",
    " ('think', 342071),\n",
    " ('about', 336843),\n",
    " ('if', 334364),\n",
    " ('me', 324107),\n",
    " ('or', 319309),\n",
    " ('at', 319235),\n",
    " ('going', 306029),\n",
    " ('as', 295102),\n",
    " ('can', 291830),\n",
    " ('one', 290678),\n",
    " ('out', 275277),\n",
    " ('really', 264046),\n",
    " ('right', 258486),\n",
    " ('get', 257908),\n",
    " ('up', 254788),\n",
    " ('when', 249582),\n",
    " ('then', 242441),\n",
    " ('people', 236501),\n",
    " ('did', 225425),\n",
    " ('got', 214973),\n",
    " ('would', 207706),\n",
    " ('she', 207256),\n",
    " ('go', 206082),\n",
    " ('from', 205767),\n",
    " ('had', 195906),\n",
    " ('now', 194102),\n",
    " (\"'ve\", 190121),\n",
    " ('how', 189243),\n",
    " ('no', 185158),\n",
    " ('some', 182741),\n",
    " ('time', 179229),\n",
    " ('kind', 178656),\n",
    " ('want', 170841),\n",
    " ('them', 169121),\n",
    " ('well', 168485),\n",
    " ('were', 166182),\n",
    " ('more', 164005),\n",
    " ('good', 158472),\n",
    " ('an', 152452),\n",
    " ('who', 148357),\n",
    " ('mean', 147539),\n",
    " ('see', 145603),\n",
    " ('his', 144276),\n",
    " ('lot', 142270),\n",
    " ('okay', 141085),\n",
    " ('back', 140301),\n",
    " ('our', 138981),\n",
    " ('say', 138289),\n",
    " ('been', 136266),\n",
    " ('will', 131598),\n",
    " ('where', 131512),\n",
    " (\"'ll\", 131387),\n",
    " ('here', 130981),\n",
    " ('thing', 130444),\n",
    " ('things', 129757),\n",
    " ('very', 126542),\n",
    " ('something', 126291),\n",
    " ('way', 126063),\n",
    " ('oh', 124976),\n",
    " ('could', 124007),\n",
    " ('has', 123704),\n",
    " ('him', 123602),\n",
    " ('their', 120746),\n",
    " ('into', 120670),\n",
    " ('little', 120398),\n",
    " ('not know', 118985),\n",
    " ('by', 118860),\n",
    " ('her', 116549),\n",
    " ('said', 115169),\n",
    " ('which', 111120),\n",
    " ('us', 109909),\n",
    " ('other', 109900),\n",
    " ('not to', 108871),\n",
    " ('first', 107927),\n",
    " ('those', 103962),\n",
    " ('these', 102243),\n",
    " ('actually', 102148),\n",
    " ('let', 101977),\n",
    " ('also', 101563),\n",
    " ('make', 101505),\n",
    " ('does', 99408),\n",
    " ('much', 97717),\n",
    " ('guys', 97609),\n",
    " ('feel', 91631),\n",
    " ('doing', 91099),\n",
    " ('love', 90736),\n",
    " ('even', 86842),\n",
    " ('why', 85685),\n",
    " ('over', 85057),\n",
    " ('look', 83951),\n",
    " ('two', 83632),\n",
    " ('podcast', 82093),\n",
    " ('day', 81897),\n",
    " ('down', 80843),\n",
    " ('being', 80782),\n",
    " ('need', 78262),\n",
    " ('come', 77296),\n",
    " ('maybe', 77295),\n",
    " ('still', 77215),\n",
    " ('life', 75402),\n",
    " ('take', 75160),\n",
    " ('year', 74405),\n",
    " ('great', 73846),\n",
    " ('off', 73804),\n",
    " ('always', 72459),\n",
    " ('game', 72296),\n",
    " ('bit', 72041),\n",
    " ('yes', 71814),\n",
    " ('through', 69633),\n",
    " ('ca', 68326),\n",
    " ('work', 67987),\n",
    " ('never', 67841),\n",
    " ('every', 67656),\n",
    " ('than', 66602),\n",
    " ('should', 66465),\n",
    " ('last', 66305),\n",
    " ('new', 66107),\n",
    " ('only', 65046),\n",
    " ('probably', 64930),\n",
    " ('not i', 64858),\n",
    " ('talk', 64837),\n",
    " ('before', 64115),\n",
    " ('put', 64056),\n",
    " ('stuff', 64024),\n",
    " ('again', 63951),\n",
    " ('god', 63632),\n",
    " ('man', 63504),\n",
    " ('not it', 62572),\n",
    " ('years', 62566),\n",
    " ('different', 62114),\n",
    " ('not a', 61942),\n",
    " ('week', 61817),\n",
    " ('not that', 61812),\n",
    " ('big', 61660),\n",
    " ('na', 61217),\n",
    " ('not the', 60229),\n",
    " ('around', 59737),\n",
    " ('gon', 59294),\n",
    " ('any', 58806),\n",
    " ('after', 58148),\n",
    " ('sure', 58073),\n",
    " ('not have', 57811),\n",
    " ('next', 57120),\n",
    " ('three', 56817),\n",
    " ('same', 55701),\n",
    " ('everything', 54664),\n",
    " ('guy', 54639),\n",
    " ('show', 54430),\n",
    " ('pretty', 53787),\n",
    " ('most', 53365),\n",
    " ('point', 53299),\n",
    " ('start', 53214),\n",
    " ('not like', 53004),\n",
    " ('give', 52572),\n",
    " ('went', 52496),\n",
    " ('thought', 52393),\n",
    " ('many', 51991),\n",
    " ('getting', 51980),\n",
    " ('talking', 51481)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-valuation",
   "metadata": {},
   "source": [
    "# Adjectives:\n",
    "[('good', 158472),\n",
    " ('little', 120398),\n",
    " ('other', 109900),\n",
    " ('much', 97717),\n",
    " ('great', 70420),\n",
    " ('last', 66305),\n",
    " ('big', 61647),\n",
    " ('different', 61615),\n",
    " ('next', 57120),\n",
    " ('same', 54776),\n",
    " ('many', 51991),\n",
    " ('new', 49781),\n",
    " ('whole', 44875),\n",
    " ('own', 41809),\n",
    " ('real', 37783),\n",
    " ('able', 34866),\n",
    " ('hard', 33712),\n",
    " ('bad', 33376),\n",
    " ('old', 29014),\n",
    " ('free', 28277),\n",
    " ('few', 28218),\n",
    " ('second', 27157),\n",
    " ('nice', 25714),\n",
    " ('live', 25662),\n",
    " ('high', 24937),\n",
    " ('important', 24218),\n",
    " ('such', 24067),\n",
    " ('true', 20846),\n",
    " ('wrong', 20042),\n",
    " ('happy', 20006),\n",
    " ('open', 18210),\n",
    " ('young', 16942),\n",
    " ('huge', 16414),\n",
    " ('full', 15458),\n",
    " ('black', 15396),\n",
    " ('certain', 15295),\n",
    " ('easy', 14913),\n",
    " ('single', 14601),\n",
    " ('social', 14550),\n",
    " ('ready', 14527),\n",
    " ('small', 13403),\n",
    " ('third', 12978),\n",
    " ('white', 11577),\n",
    " ('entire', 11112),\n",
    " ('hot', 10869),\n",
    " ('special', 10398),\n",
    " ('personal', 10243),\n",
    " ('possible', 9822),\n",
    " ('short', 9416),\n",
    " ('green', 9309),\n",
    " ('tough', 8912),\n",
    " ('not good', 8795),\n",
    " ('strong', 8636),\n",
    " ('red', 8500),\n",
    " ('positive', 8450),\n",
    " ('dead', 8212),\n",
    " ('general', 8078),\n",
    " ('similar', 7973),\n",
    " ('main', 7760),\n",
    " ('clear', 7567),\n",
    " ('low', 7546),\n",
    " ('difficult', 7533),\n",
    " ('actual', 7273),\n",
    " ('incredible', 7148),\n",
    " ('interested', 7138),\n",
    " ('final', 7055),\n",
    " ('negative', 7052),\n",
    " ('specific', 6891),\n",
    " ('healthy', 6831),\n",
    " ('extra', 6813),\n",
    " ('daily', 6616),\n",
    " ('particular', 6503),\n",
    " ('normal', 6476),\n",
    " ('serious', 6460),\n",
    " ('major', 6264),\n",
    " ('terrible', 6150),\n",
    " ('married', 6139),\n",
    " ('physical', 6125),\n",
    " ('not much', 5833),\n",
    " ('fantastic', 5722),\n",
    " ('american', 5561),\n",
    " ('local', 5394),\n",
    " ('likely', 5386),\n",
    " ('successful', 5301),\n",
    " ('available', 5296),\n",
    " ('stupid', 5242),\n",
    " ('safe', 5223),\n",
    " ('wide', 5103),\n",
    " ('overall', 5089),\n",
    " ('common', 5084),\n",
    " ('regular', 5050),\n",
    " ('comfortable', 5034),\n",
    " ('emotional', 4988),\n",
    " ('potential', 4966),\n",
    " ('massive', 4862),\n",
    " ('powerful', 4828),\n",
    " ('offensive', 4785),\n",
    " ('original', 4724),\n",
    " ('large', 4695),\n",
    " ('sudden', 4647),\n",
    " ('several', 4577),\n",
    " ('fourth', 4552),\n",
    " ('english', 4545),\n",
    " ('willing', 4453),\n",
    " ('professional', 4452),\n",
    " ('busy', 4449),\n",
    " ('natural', 4444),\n",
    " ('aware', 4332),\n",
    " ('bible', 4327),\n",
    " ('total', 4268),\n",
    " ('current', 4183),\n",
    " ('average', 4169),\n",
    " ('complete', 4138),\n",
    " ('lucky', 4126),\n",
    " ('christian', 4113),\n",
    " ('spiritual', 4106),\n",
    " ('creative', 4083),\n",
    " ('solid', 4023),\n",
    " ('heavy', 3986),\n",
    " ('not bad', 3982),\n",
    " ('scary', 3939),\n",
    " ('not able', 3913),\n",
    " ('popular', 3887),\n",
    " ('poor', 3823),\n",
    " ('curious', 3806),\n",
    " ('alive', 3802),\n",
    " ('famous', 3799),\n",
    " ('worried', 3753),\n",
    " ('sure', 3749),\n",
    " ('individual', 3698),\n",
    " ('surprised', 3676),\n",
    " ('rich', 3643),\n",
    " ('horrible', 3482),\n",
    " ('rid', 3470),\n",
    " ('previous', 3463),\n",
    " ('private', 3392),\n",
    " ('angry', 3342),\n",
    " ('due', 3332),\n",
    " ('expensive', 3331),\n",
    " ('tiny', 3330),\n",
    " ('fresh', 3300),\n",
    " ('ridiculous', 3299),\n",
    " ('nervous', 3237),\n",
    " ('basic', 3209),\n",
    " ('active', 3185),\n",
    " ('quiet', 3125),\n",
    " ('not great', 3092),\n",
    " ('financial', 3031),\n",
    " ('classic', 2993),\n",
    " ('medical', 2969),\n",
    " ('experienced', 2946),\n",
    " ('separate', 2939),\n",
    " ('modern', 2918),\n",
    " ('related', 2907),\n",
    " ('obvious', 2877),\n",
    " ('senior', 2849),\n",
    " ('recent', 2797),\n",
    " ('limited', 2771),\n",
    " ('sexual', 2739),\n",
    " ('significant', 2667),\n",
    " ('direct', 2623),\n",
    " ('french', 2584),\n",
    " ('eric', 2565),\n",
    " ('uncomfortable', 2528),\n",
    " ('familiar', 2421),\n",
    " ('various', 2364),\n",
    " ('not big', 2346),\n",
    " ('dangerous', 2337),\n",
    " ('central', 2312),\n",
    " ('flat', 2270),\n",
    " ('concerned', 2230),\n",
    " ('military', 2212),\n",
    " ('valuable', 2210),\n",
    " ('legal', 2210),\n",
    " ('former', 2184),\n",
    " ('political', 2177),\n",
    " ('british', 2067),\n",
    " ('prepared', 2063),\n",
    " ('long-term', 2050),\n",
    " ('weekly', 2046),\n",
    " ('soft', 2040),\n",
    " ('impossible', 2011),\n",
    " ('national', 2005),\n",
    " ('injured', 1991),\n",
    " ('effective', 1982),\n",
    " ('hilarious', 1981),\n",
    " ('competitive', 1948),\n",
    " ('necessary', 1946),\n",
    " ('impressive', 1933),\n",
    " ('technical', 1926),\n",
    " ('traditional', 1917),\n",
    " ('critical', 1905),\n",
    " ('not true', 1893),\n",
    " ('complex', 1860),\n",
    " ('not easy', 1847),\n",
    " ('optimal', 1836),\n",
    " ('global', 1829),\n",
    " ('not many', 1827),\n",
    " ('initial', 1821),\n",
    " ('grand', 1818)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUD\n",
    "# Om alle phrases (2 woorden combis) uit een transcript te halen\n",
    "subset_number = '1'\n",
    "folder_number = '0'\n",
    "show_uri = '10AlBXJul8JZ5bREZUXBep'\n",
    "episode_uri = '1am2bPIgTuCcAfqOY3rQZ1'\n",
    "path = 'podcast_data_no_audio/podcasts-transcripts/{}/{}/show_{}/{}.json'.format(subset_number, folder_number, show_uri, episode_uri)\n",
    "\n",
    "\n",
    "with open(path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "utterances = ''\n",
    "for utterance_number, _ in enumerate(data['results']):  \n",
    "    utterance_dict = data['results'][utterance_number]['alternatives'][0]\n",
    "    try:\n",
    "        utterance = utterance_dict['transcript'].strip()\n",
    "        utterances += ' ' + utterance\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "sentences = nltk.sent_tokenize(utterances)\n",
    "phrases = []\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    words = nltk.word_tokenize(s)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    for i in range(len(pos_tags)-1):\n",
    "        if pos_tags[i][1] in ['NNP', 'NNPS'] or pos_tags[i+1][1] in ['NNP', 'NNPS']:\n",
    "            continue\n",
    "        if pos_tags[i][1] == 'JJ':\n",
    "            if pos_tags[i+1][1] in ['NN', 'NNS']:\n",
    "                phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "            elif pos_tags[i+1][1] == 'JJ':\n",
    "                try:\n",
    "                    if pos_tags[i+2][1] not in ['NN', 'NNS']:\n",
    "                        phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "                except KeyError:\n",
    "                    phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "        elif pos_tags[i][1] in ['RB', 'RBR', 'RBS']:\n",
    "            if pos_tags[i+1][1] == 'JJ':\n",
    "                try:\n",
    "                    if pos_tags[i+2][1] not in ['NN', 'NNS']:\n",
    "                        phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "                except KeyError:\n",
    "                    phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "            elif pos_tags[i+1][1] in ['VB', 'VBD', 'VBN', 'VBG']:\n",
    "                phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "        elif pos_tags[i][1] in ['NN', 'NNS']:\n",
    "            if pos_tags[i+1][1] == 'JJ':\n",
    "                try:\n",
    "                    if pos_tags[i+2][1] not in ['NN', 'NNS']:\n",
    "                        phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "                except KeyError:\n",
    "                    phrases.append(pos_tags[i][0] + ' ' + pos_tags[i+1][0])\n",
    "print(phrases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
