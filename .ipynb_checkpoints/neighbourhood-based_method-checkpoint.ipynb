{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-dialogue",
   "metadata": {},
   "source": [
    "## Neighbourhood-based Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import operator\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import zipfile\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines whether two words are considered a phrase according to 'Thumbs Up or Thumbs Down? \n",
    "# Semantic Orientation Applied to Unsupervised Classification of Reviews' by Peter D. Turney, 2002.\n",
    "\n",
    "def is_phrase(word_index, words):\n",
    "    first_tag = nltk.pos_tag([words[word_index]])[0][1]\n",
    "    second_tag = nltk.pos_tag([words[word_index + 1]])[0][1]\n",
    "    # Considers the tag of the word after the potential phrase, if there is one.\n",
    "    try:\n",
    "        third_tag = nltk.pos_tag([words[word_index + 2]])[0][1]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    if first_tag == 'JJ':\n",
    "        if second_tag in ['NN', 'NNS']:\n",
    "            return True\n",
    "        elif second_tag == 'JJ':\n",
    "            try:\n",
    "                if third_tag not in ['NN', 'NNS']:\n",
    "                    return True\n",
    "            except NameError:\n",
    "                return True\n",
    "    elif first_tag in ['RB', 'RBR', 'RBS']:\n",
    "        if second_tag == 'JJ':\n",
    "            try:\n",
    "                if third_tag not in ['NN', 'NNS']:\n",
    "                    return True\n",
    "            except NameError:\n",
    "                return True\n",
    "        elif second_tag in ['VB', 'VBD', 'VBN', 'VBG']:\n",
    "            return True\n",
    "    elif first_tag in ['NN', 'NNS']:\n",
    "        if second_tag == 'JJ':\n",
    "            try:\n",
    "                if third_tag not in ['NN', 'NNS']:\n",
    "                    return True\n",
    "            except NameError:\n",
    "                return True\n",
    "    # If no correct combination is found, the word pair is not a phrase\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each utterance that wasn't assigned a score (either by not containing a phrase at all\n",
    "# or by not containing any with enough positive and negative words surrounding it), an average\n",
    "# of the scores of the nearest previous and next utterance is used\n",
    "\n",
    "def fill_nans(df):\n",
    "    replacements = df['predicted_score'].copy()\n",
    "    for i in range(len(df)):\n",
    "        if np.isnan(df['predicted_score'][i]):\n",
    "            new_values = []\n",
    "            j = i-1\n",
    "            while j >= 0:\n",
    "                if not np.isnan(df['predicted_score'][j]):\n",
    "                    new_values.append(1 / ((i - j) + 1) * df['predicted_score'][j])\n",
    "                    break\n",
    "                j -= 1\n",
    "            j = i+1\n",
    "            while j < len(df):\n",
    "                if not np.isnan(df['predicted_score'][j]):\n",
    "                    new_values.append(1 / ((j - i) + 1) * df['predicted_score'][j])\n",
    "                    break\n",
    "                j += 1\n",
    "            if new_values:\n",
    "                replacements[i] = np.mean(new_values)\n",
    "            else:\n",
    "                replacements[i] = 1\n",
    "    df['predicted_score'] = replacements\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns a sentiment score between 0 and 1 for each utterance in the dataframe\n",
    "\n",
    "def add_scores(df, NEAR, pos_words, neg_words):\n",
    "    # Considers the previous and upcoming NEAR words surrounding the phrase\n",
    "    utt_lens = []\n",
    "    utt_scores = []\n",
    "    all_words = []\n",
    "    utt_idx = 0\n",
    "    df['predicted_score'] = np.nan\n",
    "    \n",
    "    \n",
    "    # Gets the length of each utterance so the semantic scores are assigned to the\n",
    "    # correct utterance, also a list with all words\n",
    "    for utt in df.text:\n",
    "        utt_lens.append(len([w for w in nltk.word_tokenize(utt) if w not in ['.', ',', '?', '!', '\\'']]))\n",
    "        all_words += [w for w in nltk.word_tokenize(utt) if w not in ['.', ',', '?', '!', '\\'']]\n",
    "\n",
    "    # Checks how many positive and negative words the enitre text contains\n",
    "    word_counter = Counter([w.lower() for w in all_words])\n",
    "    pos_hits = sum([word_counter[p] for p in pos_words]) + 0.01\n",
    "    neg_hits = sum([word_counter[n] for n in neg_words]) + 0.01\n",
    "    \n",
    "    for i in range(len(all_words) - 1):\n",
    "        # Keeps track of which utterance every possible phrase is in\n",
    "        if i >= sum(utt_lens[:utt_idx+1]) or i == len(all_words) - 2:\n",
    "            # When going to a new utterance, assigns the mean of the semantic scores\n",
    "            # of each phrase in the previous utterance as the score of that utterance\n",
    "            if utt_scores:\n",
    "                df['predicted_score'][utt_idx] = np.mean(utt_scores)\n",
    "            utt_idx += 1\n",
    "            utt_scores = []\n",
    "        # Creates list of neighbouring words of each phrase found, accounting for edge cases\n",
    "        if is_phrase(i, all_words):\n",
    "            neighbourhood = []\n",
    "            if NEAR > i:\n",
    "                neighbourhood += all_words[:i]\n",
    "            else:\n",
    "                neighbourhood += all_words[i-NEAR:i]\n",
    "            if i != len(all_words) - 2:\n",
    "                try:\n",
    "                    neighbourhood += all_words[i+2:i+2+NEAR]\n",
    "                except IndexError:\n",
    "                    neighbourhood += all_words[i+2:len(words)]\n",
    "            # Counts the amount of positive and negative words in the neighbourhood\n",
    "            neighbourhood_counter = Counter([w.lower() for w in neighbourhood])\n",
    "            pos_neigh_hits = sum([neighbourhood_counter[p] for p in pos_words]) + 0.01\n",
    "            neg_neigh_hits = sum([neighbourhood_counter[n] for n in neg_words]) + 0.01\n",
    "            # Applies the function from the paper to calculate the phrase's score\n",
    "            if pos_neigh_hits > 2 and neg_neigh_hits > 2:\n",
    "                score = np.log((pos_neigh_hits * neg_hits)/ (neg_neigh_hits * pos_hits))\n",
    "                utt_scores.append(score)\n",
    "                \n",
    "\n",
    "    # Normalizes each predicted sentiment score\n",
    "    df = fill_nans(df)\n",
    "    abs_max = max([max(df['predicted_score']), -min(df['predicted_score'])])\n",
    "    if abs_max:\n",
    "        df.loc[:, 'predicted_score'] += abs_max\n",
    "        df.loc[:, 'predicted_score'] /= (2*abs_max)\n",
    "    df.loc[df.predicted_score >= 0.5, 'predicted_score'] = 1.0\n",
    "    df.loc[df.predicted_score < 0.5, 'predicted_score'] = 0.0\n",
    "        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a classification report that shows the performance of the predicted sentiment scores\n",
    "# when compared to the actual sentiment scores. Takes n randomly chosen podcast episodes with at\n",
    "# least min_len utterances from a folder with labeled csvs, or all of them if n is too large\n",
    "# and outputs the classification report.\n",
    "\n",
    "def compare_scores(labeled_folder, NEAR, pos_words, neg_words, n, min_len, output_dict):\n",
    "    # Hides warnings\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    # Gets all file names within the folder\n",
    "    filenames = [f for f in listdir(labeled_folder) if isfile(join(labeled_folder, f))]\n",
    "    np.random.shuffle(filenames)\n",
    "    dfs = []\n",
    "    c = 0\n",
    "    # Looks for up to n podcast episodes with at least min_len utterances\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv(labeled_folder + '/' + filename, sep='\\t')\n",
    "        if len(df) >= min_len:\n",
    "            dfs.append(df)\n",
    "            c += 1\n",
    "            if c == n:\n",
    "                break\n",
    "        \n",
    "    # Returns the classification report for the performance of the classification of the \n",
    "    # sentiments.\n",
    "    if len(dfs) == 1:\n",
    "        df = add_scores(df, NEAR, pos_words, neg_words)\n",
    "        return classification_report(list(df.sentiment_score.values), \n",
    "                                       list(df.predicted_score.values), zero_division=0, \n",
    "                                     output_dict=output_dict)\n",
    "    total_scores = []\n",
    "    total_predicted = []\n",
    "    for df in dfs:\n",
    "        df = add_scores(df, NEAR, pos_words, neg_words)\n",
    "        total_scores += list(df.sentiment_score.values)\n",
    "        total_predicted += list(df.predicted_score.values)\n",
    "    return classification_report(total_scores, total_predicted, zero_division=0, \n",
    "                                 output_dict=output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains the best performing NEAR value and samples of the positive and negative word lists\n",
    "# by randomly generating n parameter sets, doing a brief performance check on each, and then\n",
    "# more extensively testing those that initially performed well. Returns the most optimal \n",
    "# parameter set.\n",
    "\n",
    "def optimize_params(n, pos_words, neg_words):\n",
    "    best_params = []\n",
    "    best_accuracy = 0.0\n",
    "    good_params = []\n",
    "    \n",
    "    # Randomly generates NEAR value between 50 and 1000 and samples of the word lists\n",
    "    for i in range(n):\n",
    "        NEAR = np.random.randint(1, 20) * 50\n",
    "        sample_size = np.random.randint(len(pos_words) - 3, len(pos_words))\n",
    "        pos_sample = np.random.choice(pos_words, sample_size, replace=False)\n",
    "        neg_sample = np.random.choice(neg_words, sample_size, replace=False)\n",
    "        metrics = compare_scores('separate_csv_files', NEAR, \n",
    "                                       pos_sample, neg_sample, 1, 50, True)\n",
    "        # Parameter sets that predict both positive and negative sentiments and have decent\n",
    "        # metrics in this initial test are considered for more extensive testing\n",
    "        if '0.0' in metrics.keys() and '1.0' in metrics.keys():\n",
    "            if metrics['accuracy'] > 0.6 and metrics['0.0']['recall'] > 0.45 and metrics['0.0']['recall'] > 0.45:\n",
    "                good_params.append((NEAR, pos_sample, neg_sample))\n",
    "                \n",
    "    # All well-performing sets are then tested 30 times each, with the set that has the best\n",
    "    #average accuracy then being returned\n",
    "    for params in good_params:\n",
    "        (NEAR, pos_sample, neg_sample) = params\n",
    "        accuracy = compare_scores('separate_csv_files', NEAR, \n",
    "                                   pos_sample, neg_sample, 30, 50, True)['accuracy']\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = (NEAR, pos_sample, neg_sample)\n",
    "    return best_params, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the labeled .csv-files from Google Drive to use for validation.\n",
    "\n",
    "gdown.download('https://drive.google.com/uc?id=1aqE8yS7Lf8GfljmFEuW5pd3i5S2raW1B', 'separate_csv_files.zip', quiet=False)\n",
    "with zipfile.ZipFile('separate_csv_files.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pos_words = ['good', 'great', 'nice', 'happy', 'easy', 'yes', 'yeah', 'love',\n",
    "# #              'big', 'right', 'awesome', 'thank', 'thanks']\n",
    "# # neg_words = ['bad', 'horrible', 'tough', 'sad', 'hard', 'no', 'shit', 'hate',\n",
    "# #              'little', 'wrong', 'stupid', 'sorry']\n",
    "\n",
    "# # optimized_params, best_accuracy = optimize_params(2000, pos_words, neg_words)\n",
    "\n",
    "optimized_params = (350, ['yes', 'yeah', 'love', 'easy', 'great', 'nice', 'happy', 'awesome',\n",
    "       'thanks', 'thank'], ['little', 'wrong', 'hard', 'stupid', 'horrible', 'shit', 'sad',\n",
    "       'sorry', 'no', 'tough'])\n",
    "\n",
    "print(compare_scores('separate_csv_files', optimized_params[0], optimized_params[1],\n",
    "               optimized_params[2], 5, 150, False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
