{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Lexicon-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Bart/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all paths to the json-files of english episodes given subset number (bart: 0 , juno: 1, joris: 2)\n",
    "\n",
    "def get_paths_for_en_episodes(subset_number):\n",
    "    \"\"\"\n",
    "    Function returns list of all paths to the json-files of english \n",
    "    episodes given subset number (bart: 0 , juno: 1, joris: 2)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    path1 = 'podcast_data_no_audio/podcasts-transcripts/' + str(subset_number)\n",
    "\n",
    "    folders = listdir(path1)\n",
    "\n",
    "    if '.DS_Store' in folders:\n",
    "        folders.remove('.DS_Store')\n",
    "\n",
    "    podcast_episodes_paths = []\n",
    "\n",
    "    for letter_or_number in tqdm(folders):    \n",
    "        path2 = path1 + '/' + letter_or_number\n",
    "\n",
    "\n",
    "        for show_uri in listdir(path2):\n",
    "            path3 = path2 + '/' + show_uri\n",
    "\n",
    "            # select english shows only\n",
    "            show_metadata = metadata_df.loc[metadata_df['show_filename_prefix'] == show_uri]\n",
    "\n",
    "            if len(show_metadata['language'].unique()) > 0:\n",
    "                if 'en' in show_metadata['language'].unique()[0]:\n",
    "                    for episode_uri in listdir(path3):\n",
    "                        path4 = path3 + '/' + episode_uri\n",
    "\n",
    "                        if '.json' in path4:\n",
    "                            podcast_episodes_paths.append(path4)\n",
    "\n",
    "                \n",
    "        \n",
    "    return len(podcast_episodes_paths), podcast_episodes_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metadata_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2970c753f55e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_paths_for_en_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-2991aab63834>\u001b[0m in \u001b[0;36mget_paths_for_en_episodes\u001b[0;34m(subset_number)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# select english shows only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mshow_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'show_filename_prefix'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshow_uri\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metadata_df' is not defined"
     ]
    }
   ],
   "source": [
    "get_paths_for_en_episodes(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentiWordNet_sentiment(utterance):\n",
    "    \"\"\"\n",
    "    Returns sentiment score for a podcast utterance with tagged tokens \n",
    "    using SentiWordNet\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenize utterance\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(utterance)\n",
    "    \n",
    "    # POS tag utterance\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # assign sentiment score using SentiWordNet, including synonyms\n",
    "    tokens_sentiment_scores = []\n",
    "    for token in tagged_tokens:\n",
    "        tag = ''\n",
    "        lemma = lemmatizer.lemmatize(token[0])\n",
    "        if token[1].startswith('N'):\n",
    "            tag = 'n'\n",
    "        elif token[1].startswith('J'):\n",
    "            tag = 'a'\n",
    "        elif token[1].startswith('V'):\n",
    "            tag = 'v'\n",
    "        elif token[1].startswith('R'):\n",
    "            tag = 'r'\n",
    "        if tag != '':\n",
    "            # also get sentiments for synonyms\n",
    "            synonyms = list(swn.senti_synsets(lemma, tag)) \n",
    "            token_sentiment = 0\n",
    "            if len(synonyms) > 0:\n",
    "                for synonym in synonyms:\n",
    "                    token_sentiment += synonym.pos_score() - synonym.neg_score()\n",
    "                tokens_sentiment_scores.append(token_sentiment/len(synonyms))      \n",
    "   \n",
    "    \n",
    "    if tokens_sentiment_scores != []:\n",
    "        sentiment_score = sum(tokens_sentiment_scores)/len(tokens_sentiment_scores) \n",
    "\n",
    "        if sentiment_score >= 0:\n",
    "            return 1\n",
    "        elif sentiment_score < 0:\n",
    "            return 0\n",
    "\n",
    "    else:   \n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load binary validation dataset\n",
    "val_df = pd.read_csv('labeled_datasets/binary/binary_val.csv', sep='\\t')\n",
    "val_df.head(5)\n",
    "\n",
    "len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:17<00:00, 72.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.42      0.45       405\n",
      "         1.0       0.75      0.80      0.77       887\n",
      "\n",
      "    accuracy                           0.68      1292\n",
      "   macro avg       0.62      0.61      0.61      1292\n",
      "weighted avg       0.67      0.68      0.67      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "target_labels = val_df['sentiment_score'].values\n",
    "predicted_labels = []\n",
    "for sample in tqdm(val_df['text']):\n",
    "    predicted_sentiment = SentiWordNet_sentiment(sample)\n",
    "    predicted_labels.append(predicted_sentiment)\n",
    "\n",
    "predicted_labels = np.array(predicted_labels)  \n",
    "print(classification_report(target_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-binary validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just so people can understand what he just sai...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, I mean small businesses tough enough. So...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think Jughead needs to go, you know, it need...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't know it was like I hated it when I was...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, so and that is that is also really based...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment_score\n",
       "0  Just so people can understand what he just sai...              1.0\n",
       "1  Yeah, I mean small businesses tough enough. So...              1.0\n",
       "2  I think Jughead needs to go, you know, it need...             -1.0\n",
       "3  I don't know it was like I hated it when I was...              1.0\n",
       "4  Yeah, so and that is that is also really based...             -1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load non-binary dataset\n",
    "nb_val_df = pd.read_csv('labeled_datasets/nonbinary/nonbinary_val.csv', sep='\\t')\n",
    "nb_val_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 305/1292 [00:02<00:09, 102.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2b9b8d6887a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnb_predicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_val_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnb_predicted_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentiWordNet_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnb_predicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_predicted_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Metrics for margin = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-938538b5707a>\u001b[0m in \u001b[0;36mSentiWordNet_sentiment\u001b[0;34m(utterance)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# also get sentiments for synonyms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mtoken_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk-3.4.5-py3.7.egg/nltk/corpus/reader/sentiwordnet.py\u001b[0m in \u001b[0;36msenti_synsets\u001b[0;34m(self, string, pos)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0msentis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0msynset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynset_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0msentis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk-3.4.5-py3.7.egg/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynsets\u001b[0;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[1;32m   1582\u001b[0m             return [\n\u001b[1;32m   1583\u001b[0m                 \u001b[0mget_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1584\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1585\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk-3.4.5-py3.7.egg/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1583\u001b[0m                 \u001b[0mget_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1586\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m             ]\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk-3.4.5-py3.7.egg/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m         \u001b[0;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1912\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1913\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk-3.4.5-py3.7.egg/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mfilter_forms\u001b[0;34m(forms)\u001b[0m\n\u001b[1;32m   1894\u001b[0m             \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1898\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calculate metrics for each parameter value for the non binary margin\n",
    "nb_target_labels = list(nb_val_df['sentiment_score'].values)\n",
    "margin_values = np.linspace(0, 0.02, 21)\n",
    "for margin in margin_values:\n",
    "    nb_predicted_labels = []\n",
    "    for sample in tqdm(nb_val_df['text']):\n",
    "        nb_predicted_sentiment = SentiWordNet_sentiment(sample)\n",
    "        nb_predicted_labels.append(nb_predicted_sentiment)\n",
    "    print('Metrics for margin = {}'.format(margin))    \n",
    "    print(classification_report(nb_target_labels, nb_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VADER_sentiment_classifier(utterance, binary=True, nb_margin=0.001):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    score = analyser.polarity_scores(utterance)['compound']\n",
    "    \n",
    "    if binary == False:\n",
    "\n",
    "        if score >= nb_margin:\n",
    "            return 1\n",
    "        elif score < -nb_margin:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    elif binary == True:\n",
    "        \n",
    "        if score >= 0:\n",
    "            return 1\n",
    "        elif score < 0:\n",
    "            return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary VADER validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1292 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VADER_sentiment_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9a4eb699cf67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredicted_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVADER_sentiment_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VADER_sentiment_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "target_labels = val_df['sentiment_score'].values\n",
    "predicted_labels = []\n",
    "for sample in tqdm(val_df['text']):\n",
    "    predicted_sentiment = VADER_sentiment_classifier(sample, binary=True)\n",
    "    predicted_labels.append(predicted_sentiment)\n",
    "\n",
    "predicted_labels = np.array(predicted_labels)  \n",
    "print(classification_report(target_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-binary VADER validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:31<00:00, 40.44it/s]\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  0%|          | 3/1292 [00:00<00:55, 23.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.00      0.00      0.00       206\n",
      "         1.0       0.57      0.95      0.71       681\n",
      "\n",
      "    accuracy                           0.58      1292\n",
      "   macro avg       0.42      0.40      0.36      1292\n",
      "weighted avg       0.51      0.58      0.49      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:34<00:00, 42.31it/s]\n",
      "  0%|          | 5/1292 [00:00<00:31, 41.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:31<00:00, 40.43it/s]\n",
      "  0%|          | 5/1292 [00:00<00:28, 44.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:31<00:00, 44.33it/s]\n",
      "  0%|          | 5/1292 [00:00<00:31, 41.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:54<00:00, 39.15it/s]\n",
      "  0%|          | 3/1292 [00:00<00:47, 26.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.004\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:45<00:00, 25.82it/s]\n",
      "  0%|          | 4/1292 [00:00<00:38, 33.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.005\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [01:07<00:00, 13.25it/s]\n",
      "  0%|          | 0/1292 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.006\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [01:27<00:00, 14.81it/s]\n",
      "  0%|          | 3/1292 [00:00<00:58, 22.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [01:00<00:00, 30.44it/s]\n",
      "  0%|          | 4/1292 [00:00<00:36, 35.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:46<00:00, 27.52it/s]\n",
      "  0%|          | 5/1292 [00:00<00:27, 46.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.009000000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:53<00:00, 23.96it/s]\n",
      "  0%|          | 3/1292 [00:00<00:50, 25.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:56<00:00, 23.01it/s]\n",
      "  0%|          | 4/1292 [00:00<00:47, 27.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:46<00:00, 27.78it/s]\n",
      "  0%|          | 5/1292 [00:00<00:31, 41.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.012\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [01:01<00:00, 20.85it/s]\n",
      "  0%|          | 5/1292 [00:00<00:30, 42.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.013000000000000001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:37<00:00, 47.72it/s]\n",
      "  0%|          | 5/1292 [00:00<00:27, 47.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.014\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:33<00:00, 46.68it/s]\n",
      "  0%|          | 5/1292 [00:00<00:27, 46.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:38<00:00, 33.28it/s]\n",
      "  0%|          | 2/1292 [00:00<01:27, 14.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:54<00:00, 23.60it/s]\n",
      "  0%|          | 1/1292 [00:00<03:13,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:44<00:00, 55.91it/s]\n",
      "  0%|          | 5/1292 [00:00<00:29, 43.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.018000000000000002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:40<00:00, 28.76it/s]\n",
      "  0%|          | 3/1292 [00:00<00:43, 29.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.38       405\n",
      "         0.0       0.55      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1292/1292 [00:47<00:00, 19.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for margin = 0.02\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.68      0.26      0.37       405\n",
      "         0.0       0.54      0.41      0.47       206\n",
      "         1.0       0.62      0.90      0.74       681\n",
      "\n",
      "    accuracy                           0.62      1292\n",
      "   macro avg       0.62      0.52      0.53      1292\n",
      "weighted avg       0.63      0.62      0.58      1292\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics for each parameter value for the non binary margin\n",
    "nb_target_labels = list(nb_val_df['sentiment_score'].values)\n",
    "margin_values = np.linspace(0, 0.02, 21)\n",
    "for margin in margin_values:\n",
    "    nb_predicted_labels = []\n",
    "    for sample in tqdm(nb_val_df['text']):\n",
    "        nb_predicted_sentiment = VADER_sentiment_classifier(sample, binary=False, nb_margin=margin)\n",
    "        nb_predicted_labels.append(nb_predicted_sentiment)\n",
    "    print('Metrics for margin = {}'.format(margin))   \n",
    "    print(classification_report(nb_target_labels, nb_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (podcastDA)",
   "language": "python",
   "name": "pycharm-9e74e1a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
