{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data, filter on English podcasts and insert into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jdh/Desktop\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories of main dataset and metadata\n",
    "directory_main = 'podcast_data_no_audio/transcripts/podcasts-transcripts/2'\n",
    "directory_meta = \"podcast_data_no_audio/metadata/metadata.tsv\"\n",
    "\n",
    "# Get metadata and filter out non-english podcasts\n",
    "dftest = pd.read_csv(directory_meta,sep='\\t')\n",
    "dftest = dftest[dftest['language'].isin([\"['en']\",\"['en-US']\", \"['en-AU']\", \"['en-CA']\", \"['en-GB']\",\n",
    "       \"['en-NZ']\"])]\n",
    "\n",
    "# Create list of english podcasts\n",
    "english_id = dftest['episode_filename_prefix'].values\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df_textlist = []\n",
    "df_idlist = []\n",
    "\n",
    "# Walk through all subdirs and find jsons\n",
    "for subdir, dirs, files in os.walk(directory_main):\n",
    "    \n",
    "    for filename in files:\n",
    "        filepath = subdir + os.sep + filename\n",
    "\n",
    "        if filepath.endswith(\".json\"):\n",
    "            \n",
    "            # Create id to check if english\n",
    "            p_id = filename.replace('.json', '')\n",
    "            if p_id in english_id:\n",
    "            \n",
    "                # create empty string to append to\n",
    "                utterancestr = ''\n",
    "                \n",
    "                with open(filepath) as f:\n",
    "                    data = json.load(f)\n",
    "                for utterance_number, _ in enumerate(data['results']): \n",
    "                    utterance_dict = data['results'][utterance_number]['alternatives'][0]\n",
    "\n",
    "                    try:\n",
    "                        utterance = utterance_dict['transcript']\n",
    "                    except KeyError:\n",
    "                        utterance = '';\n",
    "\n",
    "                    utterancestr += utterance\n",
    "\n",
    "                df_textlist.append(utterancestr.strip())\n",
    "                df_idlist.append(p_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text to train model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should the saved model be called?\n",
    "model_name = \"test\"\n",
    "model_format = model_name + \".model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenized text\n",
    "df['text'] = df_textlist\n",
    "df['id'] = df_idlist\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['text_tokenized'] = df['text'].apply(tokenizer.tokenize)\n",
    "text = df['text_tokenized'].values\n",
    "\n",
    "phrases = Phrases(text, min_count=1)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- min count = 3 \n",
    ">remove most unusual words from training embeddings, like words 'ssssuuuuuuuppppppeeeeeerrrr', which actually stands for 'super', and doesn't need additional training\n",
    "\n",
    "- window = 4 \n",
    ">Word2Vec model will learn to predict given word from up to 4 words to the left, and up to 4 words to the right\n",
    "\n",
    "- size = 300 \n",
    ">size of hidden layer used to predict surroundings of embedded word, which also stands for dimensions of trained embeddings\n",
    "\n",
    "- sample = 1e-5 \n",
    ">probability baseline for subsampling most frequent words from surrounding of embedded word\n",
    "\n",
    "- negative = 20 \n",
    ">number of negative (ones that shouldn't have been predicted while modeling selected pair of words) words that will have their corresponding weights updated while training on specific training example, along with positive word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab building done!\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "modelw2v = Word2Vec(min_count=5,\n",
    "                     window=4,\n",
    "                     size=500,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "modelw2v.build_vocab(sentences)\n",
    "print(\"Vocab building done!\")\n",
    "\n",
    "modelw2v.train(sentences, total_examples=modelw2v.corpus_count, epochs=5, report_delay=1)\n",
    "print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current model for use later\n",
    "modelw2v.save(model_format)\n",
    "\n",
    "# Load the model to use now\n",
    "word_vectors = Word2Vec.load(model_format).wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the K-means algorithm and find n clusters\n",
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors.astype('double'))\n",
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Understand', 0.8800795078277588),\n",
       " ('internalize', 0.8799031972885132),\n",
       " ('compromising', 0.8732283115386963),\n",
       " ('extremely_important', 0.8664716482162476),\n",
       " ('limiting_belief', 0.8576003313064575),\n",
       " ('cognizant', 0.855897068977356),\n",
       " ('counterproductive', 0.8553060293197632),\n",
       " ('intrinsically', 0.8537331223487854),\n",
       " ('counterintuitive', 0.85052490234375),\n",
       " ('compartmentalize', 0.8488243818283081)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find words that are close to the cluster center\n",
    "word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_index = 1\n",
    "positive_cluster_center = model.cluster_centers_[positive_cluster_index]\n",
    "negative_cluster_center = model.cluster_centers_[1-positive_cluster_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign sentiment score according to cluster\n",
    "\n",
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])\n",
    "\n",
    "words['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_weighting = df['text'].copy()\n",
    "\n",
    "sentiment_dict = dict(zip(words['words'].values, words['sentiment_coeff'].values))\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(file_weighting)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(file_weighting.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this wonderful article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.title.split()))\n",
    "\n",
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)\n",
    "replaced_closeness_scores = file_weighting.title.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.title, file_weighting.rate]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'sentiment']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n",
    "replacement_df['sentiment'] = [1 if i==1 else 0 for i in replacement_df.sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = replacement_df['prediction']\n",
    "y_test = replacement_df['sentiment']\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(replacement_df.sentiment, replacement_df.prediction))\n",
    "print('Confusion Matrix')\n",
    "display(conf_matrix)\n",
    "\n",
    "test_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n",
    "\n",
    "print('\\n \\n Scores')\n",
    "scores = pd.DataFrame(data=[test_scores])\n",
    "scores.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scores = scores.T\n",
    "scores.columns = ['scores']\n",
    "display(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
