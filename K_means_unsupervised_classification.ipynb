{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-means unsupervised classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYASWo4GJXcd"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import gensim.downloader\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6q41kxeAUU0",
        "outputId": "4ff20852-c0a0-48b9-ccc5-7ae4bbc318a8"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKd2jBrsXKCA"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-r-e1V3XI9n"
      },
      "source": [
        "# Set directories of main dataset and metadata\r\n",
        "directory_main_train = 'gdrive/My Drive/Colab Notebooks/ddp/binary/binary_train.csv'\r\n",
        "directory_main_val = 'gdrive/My Drive/Colab Notebooks/ddp/binary/binary_val.csv'\r\n",
        "directory_main_full = 'gdrive/My Drive/Colab Notebooks/ddp/binary/binary_full.csv'\r\n",
        "\r\n",
        "# Should the model be saved?\r\n",
        "save_model = False\r\n",
        "model_name = \"test\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmMPKlpcJXcl"
      },
      "source": [
        "### Load the data, filter on English podcasts and insert into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lroAuaXE-MF"
      },
      "source": [
        "# Function that removes punctuation, lowercases everything (to normalize), tokenizes, and converts the labels to int\r\n",
        "def clean_data(df):\r\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "    df['text'] = df['text'].str.lower()\r\n",
        "    df['text_tokenized'] = df['text'].apply(tokenizer.tokenize)\r\n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru4wDomqEK7b"
      },
      "source": [
        "df_train = pd.read_csv(directory_main_train,sep='\\t')\n",
        "\n",
        "# Normalize and clean text\n",
        "df_train = clean_data(df_train)\n",
        "text = df_train['text_tokenized'].values\n",
        "\n",
        "optimized_terms = Phraser(Phrases(text, min_count=2))\n",
        "text_final = optimized_terms[text]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW7uAln2gqIy",
        "outputId": "82bf324c-0da2-45a2-e017-af0f74c1812c"
      },
      "source": [
        "# Build a word2vec model using the vocabulary\r\n",
        "modelw2v = Word2Vec(text_final,size=300)\r\n",
        "\r\n",
        "modelw2v.build_vocab(text_final, update=True)\r\n",
        "print(\"Vocab building done!\")\r\n",
        "\r\n",
        "modelw2v.train(text_final, total_examples=modelw2v.corpus_count, epochs=30)\r\n",
        "print(\"Training done!\")\r\n",
        "\r\n",
        "if save_model:\r\n",
        "    model_format = model_name + \".model\"\r\n",
        "\r\n",
        "    # Save the current model for use later\r\n",
        "    modelw2v.save(model_format)\r\n",
        "\r\n",
        "    # Load the model to use now\r\n",
        "    word_vectors = Word2Vec.load(model_format).wv\r\n",
        "else:\r\n",
        "    word_vectors = modelw2v.wv\r\n",
        "\r\n",
        "\r\n",
        "# Initiate the K-means algorithm and find n clusters\r\n",
        "model = KMeans(n_clusters=2, max_iter=10000, random_state=True, n_init=1000).fit(X=word_vectors.vectors.astype('double'))\r\n",
        "print('KMeans model ready!')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab building done!\n",
            "Training done!\n",
            "KMeans model ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEhmzVx7kHkP",
        "outputId": "6d5d59d7-b04e-4897-f414-0cf99984173c"
      },
      "source": [
        "print(word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None))\r\n",
        "print(word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('bethenny', 0.6718787550926208), ('guilty', 0.6651849150657654), ('uncomfortable', 0.6538792848587036), ('loyal', 0.6524480581283569), ('say_anything', 0.6517428755760193), ('focused', 0.6376308798789978), ('cowboy_hat', 0.6332029104232788), ('unbelievable', 0.6292005777359009), ('laugh', 0.6271635293960571), ('frank', 0.6247174143791199)]\n",
            "[('combat', 0.8252910375595093), ('free_agency', 0.8121446371078491), ('team_rules', 0.8059823513031006), ('previous', 0.8020377159118652), ('creatures', 0.8013936281204224), ('rate', 0.7976237535476685), ('our_community', 0.7950810194015503), ('kicks', 0.7943021059036255), ('chompers', 0.793852686882019), ('uk', 0.793601930141449)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkowjshRJXcv",
        "scrolled": false
      },
      "source": [
        "# Set the cluster positions\n",
        "positive_cluster_index = 0\n",
        "positive_cluster_center = model.cluster_centers_[positive_cluster_index]\n",
        "negative_cluster_center = model.cluster_centers_[1-positive_cluster_index]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU5r4ZzoJXcw"
      },
      "source": [
        "def create_df_dict():\n",
        "\n",
        "    # Create vectors for each word\n",
        "    metrics_df = pd.DataFrame(word_vectors.vocab.keys())\n",
        "    metrics_df.columns = ['words']\n",
        "\n",
        "    # Assign words to a cluster using Sklearn's predict\n",
        "    metrics_df['vectors'] = metrics_df['words'].apply(lambda x: word_vectors[f'{x}'])\n",
        "    metrics_df['cluster'] = metrics_df['vectors'].apply(lambda x: model.predict([np.array(x)]))\n",
        "\n",
        "    # Unpack the values from list\n",
        "    metrics_df['cluster'] = metrics_df['cluster'].apply(lambda x: x[0])\n",
        "\n",
        "    # Assign words to cluster\n",
        "    metrics_df['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in metrics_df['cluster']]\n",
        "\n",
        "    # Assign the inverse distance to the closest cluster to each word\n",
        "    metrics_df['distance'] = metrics_df.apply(lambda x: 1/(model.transform([x['vectors']]).min()), axis=1)\n",
        "\n",
        "    # Calculate the sentiment coefficient\n",
        "    metrics_df['sentiment_coeff'] = metrics_df['distance'] * metrics_df['cluster_value']\n",
        "\n",
        "    sentiment_dict = dict(zip(metrics_df['words'].values, metrics_df['sentiment_coeff'].values))\n",
        "\n",
        "    return metrics_df, sentiment_dict"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZSLpa2JXcw"
      },
      "source": [
        "### ---Tf-idf weighting---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc34vUoJJXcw"
      },
      "source": [
        "# Load in the validation set and clean like the training set\r\n",
        "df_val = pd.read_csv(directory_main_val,sep='\\t')\r\n",
        "df_val = clean_data(df_val)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRs-QDaySA7",
        "outputId": "d62169db-bc13-4f2f-cc8c-233f4ac2677b"
      },
      "source": [
        "# Vectorize the sequences\r\n",
        "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\r\n",
        "tfidf.fit(df_val['text'])\r\n",
        "\r\n",
        "# Get the names of each feature\r\n",
        "features = pd.Series(tfidf.get_feature_names())\r\n",
        "\r\n",
        "# Transform the text into their respective TF-IDF values\r\n",
        "transformed = tfidf.transform(df_val['text'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPcQBKt3JXcx"
      },
      "source": [
        "metrics_df, sentiment_dict = create_df_dict()\n",
        "\n",
        "# Create a dictionary of every word and its corresponding TF-IDF value\n",
        "def create_tfidf_dictionary(x, transformed_file, features):\n",
        "    vector_coo = transformed_file[x.name].tocoo()\n",
        "    vector_coo.col = features.iloc[vector_coo.col].values\n",
        "    return dict(zip(vector_coo.col, vector_coo.data))\n",
        "\n",
        "def replace_tfidf_words(x, transformed_file, features):\n",
        "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
        "    return list(map(lambda y:dictionary[f'{y}'], x['text'].split()))\n",
        "\n",
        "# Replaces a word with its respective sentiment value\n",
        "def replace_sentiment_words(word, sentiment_dict):\n",
        "    try:\n",
        "        return sentiment_dict[word]\n",
        "    except KeyError:\n",
        "        return 0\n",
        "\n",
        "replaced_tfidf_scores = df_val.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)\n",
        "replaced_closeness_scores = df_val['text'].apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))\n",
        "\n",
        "# Create new dataframe for final calculations\n",
        "df_kmeans = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, df_val['text'], df_val['sentiment_score']]).T\n",
        "df_kmeans.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'sentiment_score']\n",
        "\n",
        "# Take the dot product to determine if a segment is mostly positive or mostly negative\n",
        "df_kmeans['prediction'] = df_kmeans.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
        "\n",
        "# Predict the label and convert to the same datatype\n",
        "df_kmeans['prediction'] = (df_kmeans['prediction']>=0).astype('int8')\n",
        "df_kmeans['sentiment_score'] = df_kmeans['sentiment_score'].astype('int8')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9IrWXAJXcy"
      },
      "source": [
        "### ---Performance Metrics---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "146unX5xJXcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249549b8-70fc-4ae2-8894-564b6db7d19d"
      },
      "source": [
        "y_true_kmeans = df_kmeans['sentiment_score']\n",
        "y_pred_kmeans = df_kmeans['prediction']\n",
        "\n",
        "# Display the final scores\n",
        "print('Confusion Matrix\\n',confusion_matrix(y_true_kmeans,y_pred_kmeans))\n",
        "print(classification_report(y_true_kmeans, y_pred_kmeans))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            " [[172 233]\n",
            " [447 440]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.42      0.34       405\n",
            "           1       0.65      0.50      0.56       887\n",
            "\n",
            "    accuracy                           0.47      1292\n",
            "   macro avg       0.47      0.46      0.45      1292\n",
            "weighted avg       0.54      0.47      0.49      1292\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IFu91t7LuO9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}