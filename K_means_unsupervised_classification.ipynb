{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-means unsupervised classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYASWo4GJXcd"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import gensim.downloader\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6q41kxeAUU0",
        "outputId": "28ddf4be-ec6d-4ca3-fe0f-e40a8baaaf47"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKd2jBrsXKCA"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-KBqqrIM3jK"
      },
      "source": [
        "## 1 Introduction\r\n",
        "\r\n",
        "A podcast is an audio file that contains dialogue of one or more participants, which can be easily downloaded and listened to. The term podcast was first coined by the columnist and journalist Ben Hammersley in February 2004 as a way to combine the terms *'i-Pod'* and *'broadcasting'*. \r\n",
        "In 2005, the American company *Apple* released a new version of iTunes which provided a centralized platform for podcasts to be uploaded to and downloaded from.\r\n",
        "\r\n",
        "\r\n",
        "It is a traditional task in A.I. to predict the general tone of a sentence or sequence. For example, a company owner might attempt to filter positive reviews from negative reviews, but this can be taxing to do by hand. Thus, the owner could decide to have the reviews analyzed by an automated agent. The conventional name for an agent performing such a task is sentiment analysis.\r\n",
        "\r\n",
        "\r\n",
        "Podcasts are interesting because they come in various shapes and sizes. Some are serious political talkshows while others are completely based on fiction. An interesting task would therefore be to detect the general mood throughout a podcast in order to learn more about the general composition of its contents. Therefore, this report aims at finding patterns in a single podcast episode and between podcast episodes. \r\n",
        "\r\n",
        "\r\n",
        "**- Leuk pakkend einde van introductie**\r\n",
        "**- Research question(s)**\r\n",
        "The high popularity\r\n",
        "\r\n",
        "RQ: What sentiment patterns can be found in podcasts episodes? \\\\\r\n",
        "SubRQ1: What sentiment patterns can be found within a single podcast episode? \\\\\r\n",
        "SubRQ2: What sentiment patterns can be found in podcast episodes in general?\r\n",
        "\r\n",
        "\r\n",
        "**- Several methods will be examined**\r\n",
        "\r\n",
        "**- Pattern mining**\r\n",
        "\r\n",
        "**- Overzicht van wat er wordt besproken**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-r-e1V3XI9n"
      },
      "source": [
        "# Set directories of main dataset and metadata\r\n",
        "directory_main_train = 'gdrive/My Drive/Colab Notebooks/ddp/binary/binary_train.csv'\r\n",
        "directory_main_val = 'gdrive/My Drive/Colab Notebooks/ddp/binary/binary_val.csv'\r\n",
        "directory_main_full = 'gdrive/My Drive/Colab Notebooks/ddp/binary/binary_full.csv'\r\n",
        "\r\n",
        "# Should the model be saved?\r\n",
        "save_model = False\r\n",
        "model_name = \"test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmMPKlpcJXcl"
      },
      "source": [
        "### Load the data, filter on English podcasts and insert into dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lroAuaXE-MF"
      },
      "source": [
        "# Function that removes punctuation, lowercases everything (to normalize), tokenizes, and converts the labels to int\r\n",
        "def clean_data(df):\r\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "    df['text'] = df['text'].str.lower()\r\n",
        "    df['text_tokenized'] = df['text'].apply(tokenizer.tokenize)\r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru4wDomqEK7b",
        "outputId": "f2ac86d9-9efd-429c-a3df-0a8f5bbab4d8"
      },
      "source": [
        "df_train = pd.read_csv(directory_main_train,sep='\\t')\n",
        "\n",
        "# Normalize and clean text\n",
        "df_train = clean_data(df_train)\n",
        "text = df_train['text_tokenized'].values\n",
        "\n",
        "# Detect common phrases in the text (n-grams)\n",
        "terms = Phrases(text, min_count=2)\n",
        "\n",
        "# Extract bigrams\n",
        "optimized_terms = Phraser(terms)\n",
        "text_final = optimized_terms[text]\n",
        "print('Text ready!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW7uAln2gqIy",
        "outputId": "7c91d569-dbcf-4c0b-be58-c16b858ccb5a"
      },
      "source": [
        "# Build a word2vec model using the vocabulary\r\n",
        "modelw2v = Word2Vec(text_final,size=300)\r\n",
        "\r\n",
        "modelw2v.build_vocab(text_final, update=True)\r\n",
        "print(\"Vocab building done!\")\r\n",
        "\r\n",
        "modelw2v.train(text_final, total_examples=modelw2v.corpus_count, epochs=30)\r\n",
        "print(\"Training done!\")\r\n",
        "\r\n",
        "if save_model:\r\n",
        "    model_format = model_name + \".model\"\r\n",
        "\r\n",
        "    # Save the current model for use later\r\n",
        "    modelw2v.save(model_format)\r\n",
        "\r\n",
        "    # Load the model to use now\r\n",
        "    word_vectors = Word2Vec.load(model_format).wv\r\n",
        "else:\r\n",
        "    word_vectors = modelw2v.wv\r\n",
        "\r\n",
        "\r\n",
        "# Initiate the K-means algorithm and find n clusters\r\n",
        "model = KMeans(n_clusters=2, max_iter=10000, random_state=True, n_init=1000).fit(X=word_vectors.vectors.astype('double'))\r\n",
        "print('KMeans model ready!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab building done!\n",
            "Training done!\n",
            "KMeans model ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEhmzVx7kHkP",
        "outputId": "f67bf414-afce-42b2-bd74-ef5a2c146ef9"
      },
      "source": [
        "print(word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None))\r\n",
        "print(word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('uk', 0.8665963411331177), ('combat', 0.8614566326141357), ('creatures', 0.853045642375946), ('free_agency', 0.8391095399856567), ('grass', 0.8373022079467773), ('incentives', 0.8346552848815918), ('safety', 0.8314654231071472), ('hollywood', 0.8295350074768066), ('wealth', 0.8280388116836548), ('champagne', 0.8279585838317871)]\n",
            "[('say_anything', 0.7285462021827698), ('guilty', 0.7254054546356201), ('admit', 0.672544002532959), ('fine', 0.6677496433258057), ('laugh', 0.6655440926551819), ('um', 0.6292319893836975), ('hmm', 0.6247437596321106), ('convinced', 0.6223713755607605), ('uncomfortable', 0.6217079758644104), ('wrong', 0.6200493574142456)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkowjshRJXcv",
        "scrolled": false
      },
      "source": [
        "# Set the cluster positions\n",
        "positive_cluster_index = 0\n",
        "positive_cluster_center = model.cluster_centers_[positive_cluster_index]\n",
        "negative_cluster_center = model.cluster_centers_[1-positive_cluster_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU5r4ZzoJXcw"
      },
      "source": [
        "# Create vectors for each word\n",
        "words = pd.DataFrame(word_vectors.vocab.keys())\n",
        "words.columns = ['words']\n",
        "\n",
        "# Assign words to a cluster using Sklearn's predict\n",
        "words['vectors'] = words['words'].apply(lambda x: word_vectors[f'{x}'])\n",
        "words['cluster'] = words['vectors'].apply(lambda x: model.predict([np.array(x)]))\n",
        "\n",
        "# Unpack the values from list\n",
        "words['cluster'] = words['cluster'].apply(lambda x: x[0])\n",
        "\n",
        "# Assign words to cluster\n",
        "words['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words['cluster']]\n",
        "\n",
        "# Assign the inverse distance to the closest cluster to each word\n",
        "words['distance'] = words.apply(lambda x: 1/(model.transform([x['vectors']]).min()), axis=1)\n",
        "\n",
        "# Calculate the sentiment coefficient\n",
        "words['sentiment_coeff'] = words['distance'] * words['cluster_value']\n",
        "\n",
        "sentiment_dict = dict(zip(words['words'].values, words['sentiment_coeff'].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZSLpa2JXcw"
      },
      "source": [
        "### ---Tf-idf weighting---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc34vUoJJXcw"
      },
      "source": [
        "# Load in the validation set and clean like the training set\r\n",
        "df_val = pd.read_csv(directory_main_val,sep='\\t')\r\n",
        "df_val = clean_data(df_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRs-QDaySA7",
        "outputId": "3dcd232e-b385-4e2d-e792-f7d29bf14a61"
      },
      "source": [
        "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\r\n",
        "tfidf.fit(df_val['text'])\r\n",
        "features = pd.Series(tfidf.get_feature_names())\r\n",
        "transformed = tfidf.transform(df_val['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPcQBKt3JXcx"
      },
      "source": [
        "def create_tfidf_dictionary(x, transformed_file, features):\n",
        "    vector_coo = transformed_file[x.name].tocoo()\n",
        "    vector_coo.col = features.iloc[vector_coo.col].values\n",
        "    return dict(zip(vector_coo.col, vector_coo.data))\n",
        "\n",
        "\n",
        "def replace_tfidf_words(x, transformed_file, features):\n",
        "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
        "    return list(map(lambda y:dictionary[f'{y}'], x['text'].split()))\n",
        "\n",
        "# Replaces a word with its respective sentiment value\n",
        "def replace_sentiment_words(word, sentiment_dict):\n",
        "    try:\n",
        "        return sentiment_dict[word]\n",
        "    except KeyError:\n",
        "        return 0\n",
        "\n",
        "replaced_tfidf_scores = df_val.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)\n",
        "replaced_closeness_scores = df_val['text'].apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))\n",
        "\n",
        "# Create new dataframe for final calculations\n",
        "df_kmeans = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, df_val['text'], df_val['sentiment_score']).T\n",
        "df_kmeans.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'sentiment_score']\n",
        "\n",
        "# Take the dot product to determine if a segment is mostly positive or mostly negative\n",
        "df_kmeans['prediction'] = df_kmeans.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
        "\n",
        "# Predict the label and convert to the same datatype\n",
        "df_kmeans['prediction'] = (df_kmeans['prediction']>=0).astype('int8')\n",
        "df_kmeans['sentiment_score'] = df_kmeans['sentiment_score'].astype('int8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9IrWXAJXcy"
      },
      "source": [
        "### ---Performance Metrics---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "146unX5xJXcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc648e8-9b49-4ffa-faac-45fc22326add"
      },
      "source": [
        "y_true_kmeans = df['sentiment_score']\n",
        "y_pred_kmeans = df['prediction']\n",
        "\n",
        "# Display the final scores\n",
        "print('Confusion Matrix\\n',confusion_matrix(y_true_kmeans,y_pred_kmeans))\n",
        "print(classification_report(y_true_kmeans, y_pred_kmeans))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            " [[233 172]\n",
            " [438 449]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.58      0.43       405\n",
            "           1       0.72      0.51      0.60       887\n",
            "\n",
            "    accuracy                           0.53      1292\n",
            "   macro avg       0.54      0.54      0.51      1292\n",
            "weighted avg       0.61      0.53      0.54      1292\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}